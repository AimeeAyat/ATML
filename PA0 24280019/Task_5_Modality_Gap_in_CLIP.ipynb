{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2zC4WM7-gQlM",
        "outputId": "4a069195-c95a-4290-d26e-8711f8877bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-zh99f9cx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-zh99f9cx\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (0.5.9.post2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision ftfy regex tqdm matplotlib umap-learn scikit-learn scipy git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import github clip\n",
        "!pip install open_clip_torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xi8FHzDQlWbN",
        "outputId": "074b3d91-8e7e-4d19-cd8b-623c511876d5"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.1.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.23.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.6.2)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.19)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.1.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, random, json\n",
        "from typing import List, Dict, Tuple"
      ],
      "metadata": {
        "id": "KTgXnQme9Aae"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qOD0uV1w9AXn"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "byov-Ky29AU1"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import umap\n",
        "    HAVE_UMAP = True\n",
        "except Exception:\n",
        "    HAVE_UMAP = False"
      ],
      "metadata": {
        "id": "KEN-TPkP9ARB"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.linalg import orthogonal_procrustes"
      ],
      "metadata": {
        "id": "IxIpZXQX9AOB"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting Reproducability of code & device\n",
        "def set_seed(seed=1337):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "ZwdcNgcb9AK_"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"artifacts\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "AjwH_8xk9AIC"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oUIqghrN9AE_",
        "outputId": "2427a1ff-3f65-4b73-9949-771df7b51e1d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mqlT4TCo9-5v",
        "outputId": "c81cd678-f60f-4488-9df5-11abc836db27"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIP(\n",
              "  (visual): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (resblocks): Sequential(\n",
              "      (0): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (1): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (2): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (3): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (4): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (5): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (6): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (7): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (8): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (9): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (10): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (11): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (token_embedding): Embedding(49408, 512)\n",
              "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HAVE_UMAP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQbG6CREOKm4",
        "outputId": "60a9ca94-4b95-4688-e5e7-da64b03590bf"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = datasets.STL10(root=\"data\", split=\"test\", download=True, transform=clip_preprocess)\n",
        "test_loader = DataLoader(test, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "duVxhCK29ACU"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classes used in STL-10\n",
        "CLASSES = [\"airplane\",\"bird\",\"car\",\"cat\",\"deer\",\"dog\",\"horse\",\"monkey\",\"ferry\",\"truck\"]"
      ],
      "metadata": {
        "id": "r8OpOi19_axG"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_sets() -> Dict[str, List[str]]:\n",
        "\n",
        "    # plain labels\n",
        "    plain = [f\"{c}\" for c in CLASSES]\n",
        "\n",
        "    # short template\n",
        "    short = [f\"a photo of a {c}\" for c in CLASSES]\n",
        "\n",
        "    #  descriptive variants\n",
        "    TEMPLATES = [\n",
        "        \"a photo of a {}.\",\n",
        "        \"a close-up photo of a {}.\",\n",
        "        \"a bright photo of a {}.\",\n",
        "        \"a cropped photo of the {}.\",\n",
        "        \"a photo of the small {}.\",\n",
        "        \"a close-up of the {}.\",\n",
        "        \"a low resolution photo of a {}.\",\n",
        "        \"a high resolution photo of a {}.\",\n",
        "        \"a picture of one {}.\",\n",
        "        \"a photo of many {}.\",\n",
        "        \"a photograph of a big {}.\",\n",
        "        \"a JPEG photo of a {}.\"\n",
        "    ]\n",
        "\n",
        "    # We'll average text features across templates per class.\n",
        "    variants = []\n",
        "    for c in CLASSES:\n",
        "        variants.append(\"|\".join([t.format(c) for t in TEMPLATES]))\n",
        "\n",
        "    return {\n",
        "        \"plain\": plain,\n",
        "        \"short\": short,\n",
        "        \"variants\": variants,  # pipe-separated template strings per class\n",
        "    }\n",
        "\n",
        "    print(list(prompt_sets()))\n",
        "\n",
        "prompt_sets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hhh6aoJ7_au5",
        "outputId": "06f3101a-93d8-4918-cc50-ac20b8740771"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'plain': ['airplane',\n",
              "  'bird',\n",
              "  'car',\n",
              "  'cat',\n",
              "  'deer',\n",
              "  'dog',\n",
              "  'horse',\n",
              "  'monkey',\n",
              "  'ferry',\n",
              "  'truck'],\n",
              " 'short': ['a photo of a airplane',\n",
              "  'a photo of a bird',\n",
              "  'a photo of a car',\n",
              "  'a photo of a cat',\n",
              "  'a photo of a deer',\n",
              "  'a photo of a dog',\n",
              "  'a photo of a horse',\n",
              "  'a photo of a monkey',\n",
              "  'a photo of a ferry',\n",
              "  'a photo of a truck'],\n",
              " 'variants': ['a photo of a airplane.|a close-up photo of a airplane.|a bright photo of a airplane.|a cropped photo of the airplane.|a photo of the small airplane.|a close-up of the airplane.|a low resolution photo of a airplane.|a high resolution photo of a airplane.|a picture of one airplane.|a photo of many airplane.|a photograph of a big airplane.|a JPEG photo of a airplane.',\n",
              "  'a photo of a bird.|a close-up photo of a bird.|a bright photo of a bird.|a cropped photo of the bird.|a photo of the small bird.|a close-up of the bird.|a low resolution photo of a bird.|a high resolution photo of a bird.|a picture of one bird.|a photo of many bird.|a photograph of a big bird.|a JPEG photo of a bird.',\n",
              "  'a photo of a car.|a close-up photo of a car.|a bright photo of a car.|a cropped photo of the car.|a photo of the small car.|a close-up of the car.|a low resolution photo of a car.|a high resolution photo of a car.|a picture of one car.|a photo of many car.|a photograph of a big car.|a JPEG photo of a car.',\n",
              "  'a photo of a cat.|a close-up photo of a cat.|a bright photo of a cat.|a cropped photo of the cat.|a photo of the small cat.|a close-up of the cat.|a low resolution photo of a cat.|a high resolution photo of a cat.|a picture of one cat.|a photo of many cat.|a photograph of a big cat.|a JPEG photo of a cat.',\n",
              "  'a photo of a deer.|a close-up photo of a deer.|a bright photo of a deer.|a cropped photo of the deer.|a photo of the small deer.|a close-up of the deer.|a low resolution photo of a deer.|a high resolution photo of a deer.|a picture of one deer.|a photo of many deer.|a photograph of a big deer.|a JPEG photo of a deer.',\n",
              "  'a photo of a dog.|a close-up photo of a dog.|a bright photo of a dog.|a cropped photo of the dog.|a photo of the small dog.|a close-up of the dog.|a low resolution photo of a dog.|a high resolution photo of a dog.|a picture of one dog.|a photo of many dog.|a photograph of a big dog.|a JPEG photo of a dog.',\n",
              "  'a photo of a horse.|a close-up photo of a horse.|a bright photo of a horse.|a cropped photo of the horse.|a photo of the small horse.|a close-up of the horse.|a low resolution photo of a horse.|a high resolution photo of a horse.|a picture of one horse.|a photo of many horse.|a photograph of a big horse.|a JPEG photo of a horse.',\n",
              "  'a photo of a monkey.|a close-up photo of a monkey.|a bright photo of a monkey.|a cropped photo of the monkey.|a photo of the small monkey.|a close-up of the monkey.|a low resolution photo of a monkey.|a high resolution photo of a monkey.|a picture of one monkey.|a photo of many monkey.|a photograph of a big monkey.|a JPEG photo of a monkey.',\n",
              "  'a photo of a ferry.|a close-up photo of a ferry.|a bright photo of a ferry.|a cropped photo of the ferry.|a photo of the small ferry.|a close-up of the ferry.|a low resolution photo of a ferry.|a high resolution photo of a ferry.|a picture of one ferry.|a photo of many ferry.|a photograph of a big ferry.|a JPEG photo of a ferry.',\n",
              "  'a photo of a truck.|a close-up photo of a truck.|a bright photo of a truck.|a cropped photo of the truck.|a photo of the small truck.|a close-up of the truck.|a low resolution photo of a truck.|a high resolution photo of a truck.|a picture of one truck.|a photo of many truck.|a photograph of a big truck.|a JPEG photo of a truck.']}"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode text prompts into class prototypes\n",
        "@torch.no_grad()\n",
        "def build_text_prototypes(model, prompt_sets: Dict[str, List[str]]) -> Dict[str, torch.Tensor]:\n",
        "    out = {}\n",
        "    for key, prompts in prompt_sets.items():\n",
        "        class_feats = []\n",
        "        for entry in prompts:\n",
        "            # entry either a single prompt (plain/short) or a pipe-joined ensemble string (variants)\n",
        "            if \"|\" in entry:\n",
        "                texts = entry.split(\"|\")\n",
        "                tokens = clip.tokenize(texts).to(device)\n",
        "                txt_feat = model.encode_text(tokens)  # [n_templates, d]\n",
        "                txt_feat = F.normalize(txt_feat, dim=-1)\n",
        "                proto = txt_feat.mean(dim=0, keepdim=True)  # average ensemble\n",
        "            else:\n",
        "                tokens = clip.tokenize([entry]).to(device)\n",
        "                proto = F.normalize(model.encode_text(tokens), dim=-1)\n",
        "            class_feats.append(proto)\n",
        "        # shape: [num_classes, d]\n",
        "        out[key] = torch.cat(class_feats, dim=0)\n",
        "    return out  # dict of [C, D]"
      ],
      "metadata": {
        "id": "PxVus2-I_asJ"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_prototypes=build_text_prototypes(model, prompt_sets=prompt_sets())\n",
        "text_prototypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-gDDbSXI_apU",
        "outputId": "54fce63a-df84-4a99-b175-da1ad081939f"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'plain': tensor([[ 0.0059, -0.0175, -0.0041,  ...,  0.0177, -0.0110, -0.0175],\n",
              "         [ 0.0213,  0.0039,  0.0057,  ..., -0.0332, -0.0021, -0.0254],\n",
              "         [ 0.0060,  0.0097,  0.0036,  ..., -0.0225, -0.0248,  0.0104],\n",
              "         ...,\n",
              "         [ 0.0056,  0.0012, -0.0085,  ..., -0.0209,  0.0038,  0.0008],\n",
              "         [ 0.0350, -0.0259, -0.0195,  ..., -0.0206,  0.0156,  0.0260],\n",
              "         [ 0.0222,  0.0051, -0.0071,  ..., -0.0005, -0.0157,  0.0013]],\n",
              "        device='cuda:0', dtype=torch.float16),\n",
              " 'short': tensor([[ 0.0128,  0.0265,  0.0104,  ..., -0.0315,  0.0071, -0.0219],\n",
              "         [ 0.0229,  0.0381, -0.0079,  ..., -0.0594,  0.0048, -0.0292],\n",
              "         [ 0.0071,  0.0141, -0.0105,  ..., -0.0375, -0.0331,  0.0005],\n",
              "         ...,\n",
              "         [ 0.0102,  0.0345, -0.0019,  ..., -0.0366,  0.0031, -0.0028],\n",
              "         [ 0.0314, -0.0218, -0.0071,  ..., -0.0060,  0.0069,  0.0253],\n",
              "         [ 0.0123,  0.0341, -0.0019,  ..., -0.0508, -0.0196, -0.0007]],\n",
              "        device='cuda:0', dtype=torch.float16),\n",
              " 'variants': tensor([[ 0.0101,  0.0224, -0.0007,  ..., -0.0208,  0.0206, -0.0178],\n",
              "         [ 0.0170,  0.0373, -0.0159,  ..., -0.0458,  0.0042, -0.0327],\n",
              "         [-0.0012,  0.0186, -0.0247,  ..., -0.0290, -0.0284, -0.0003],\n",
              "         ...,\n",
              "         [ 0.0031,  0.0323, -0.0105,  ..., -0.0250,  0.0085, -0.0018],\n",
              "         [ 0.0249, -0.0204, -0.0033,  ..., -0.0024,  0.0145,  0.0309],\n",
              "         [ 0.0037,  0.0360, -0.0123,  ..., -0.0385, -0.0111,  0.0018]],\n",
              "        device='cuda:0', dtype=torch.float16)}"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check size of tensors 10 for classes and 512 for clip embeddings of each class\n",
        "print(len(text_prototypes))\n",
        "print(text_prototypes[\"plain\"].shape)\n",
        "print(text_prototypes[\"short\"].shape)\n",
        "print(text_prototypes[\"variants\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M4HAn4j_am9",
        "outputId": "1360148f-c687-4a42-87d7-82dd911ade46"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "torch.Size([10, 512])\n",
            "torch.Size([10, 512])\n",
            "torch.Size([10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding images\n",
        "@torch.no_grad()\n",
        "def encode_images(model, loader) -> torch.Tensor:\n",
        "    image_feats = []\n",
        "    for imgs, _ in tqdm(loader, desc=\"Encoding images\"):\n",
        "        imgs = imgs.to(device)\n",
        "        feats = model.encode_image(imgs)\n",
        "        feats = F.normalize(feats, dim=-1)\n",
        "        image_feats.append(feats.float().cpu())\n",
        "    return torch.cat(image_feats, dim=0)  # [N, D]"
      ],
      "metadata": {
        "id": "PYl7WtmuMBDG"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_images = encode_images(model, test_loader)\n",
        "encoded_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G6vOxXuoMBAd",
        "outputId": "c55897ba-6c52-49fd-f904-873514e90336"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding images: 100%|| 250/250 [00:39<00:00,  6.31it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.4327e-03,  2.2202e-02,  3.0212e-03,  ...,  7.8674e-02,\n",
              "         -8.2550e-03,  2.9709e-02],\n",
              "        [-8.5602e-03,  2.7130e-02, -8.6606e-05,  ...,  5.6213e-02,\n",
              "          1.1093e-02,  1.8326e-02],\n",
              "        [-1.3489e-02, -8.3971e-04, -3.1853e-03,  ...,  5.8868e-02,\n",
              "         -2.5574e-02, -1.8509e-02],\n",
              "        ...,\n",
              "        [ 3.0746e-02, -6.2065e-03, -2.8885e-02,  ...,  5.6946e-02,\n",
              "          2.1915e-03, -4.6425e-03],\n",
              "        [-1.0956e-02, -9.5320e-04, -5.4230e-02,  ...,  6.7566e-02,\n",
              "         -2.1545e-02,  1.4524e-03],\n",
              "        [-9.6664e-03, -2.8427e-02,  4.9019e-04,  ...,  4.5563e-02,\n",
              "          4.0512e-03, -2.1408e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoded_images))\n",
        "print(encoded_images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI3H9IAtMA90",
        "outputId": "a0ecb7aa-f7eb-4133-e4f9-e6555c3f9b17"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000\n",
            "torch.Size([8000, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot accuracy\n",
        "@torch.no_grad()\n",
        "def zero_shot_eval(image_feats: torch.Tensor, text_protos: torch.Tensor, labels: np.ndarray) -> float:\n",
        "    # cosine similarity via dot after L2 norm above\n",
        "    image_feats=image_feats.to(device).half() # Convert image_feats to half precision\n",
        "    text_protos=text_protos.to(device)\n",
        "    logits = image_feats @ text_protos.T  # [N, C]\n",
        "    print(image_feats.dtype, text_protos.dtype)\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    acc = (preds == labels).mean() * 100.0\n",
        "    return float(acc), preds"
      ],
      "metadata": {
        "id": "phzpsZsmMgM1"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_evaluation_plain=zero_shot_eval(encoded_images, text_prototypes['plain'], test.labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVaUYo_RMgKK",
        "outputId": "42f0ce9f-602e-463e-cf94-fe754d24bbb3"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16 torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_plain , _ = zero_shot_evaluation_plain\n",
        "print(f\"Zero-shot accuracy: {accuracy_plain:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3h_JFwuMgH7",
        "outputId": "39675138-0589-42d0-8a18-c1dd19e26e2d"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot accuracy: 95.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_evaluation_short=zero_shot_eval(encoded_images, text_prototypes['short'], test.labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InQKwSQOQSQu",
        "outputId": "34bbfecb-b981-4dd6-f89c-7c812ff20942"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16 torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_short , _ = zero_shot_evaluation_short\n",
        "print(f\"Zero-shot accuracy: {accuracy_short:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g4NyqhUQfLU",
        "outputId": "0a437fd2-53b2-4327-8229-3aa137c828b9"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot accuracy: 96.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_evaluation_variants=zero_shot_eval(encoded_images, text_prototypes['variants'], test.labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1i3xi0UQk5Y",
        "outputId": "11c9e5ac-e87f-48fc-b18a-7bb8b4e27be4"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16 torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_variants , _ = zero_shot_evaluation_variants\n",
        "print(f\"Zero-shot accuracy: {accuracy_variants:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBMKjcOiQk3H",
        "outputId": "2ade4524-4a9a-459a-f800-d69522ef4053"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot accuracy: 96.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_prompts(model, test_loader, test_ds, text_proto_dict):\n",
        "    # Encode all test images once\n",
        "    image_feats = encode_images(model, test_loader)  # [N,D]\n",
        "    labels = np.array(test_ds.labels)  # STL10 stores test labels in .labels\n",
        "\n",
        "    results = {}\n",
        "    for name, protos in text_proto_dict.items():\n",
        "        acc, preds = zero_shot_eval(image_feats, protos.to(image_feats.device), labels)\n",
        "        results[name] = {\"acc\": acc}\n",
        "        np.save(f\"artifacts/preds_{name}.npy\", preds)\n",
        "    np.save(\"artifacts/image_feats.npy\", image_feats.numpy())\n",
        "    np.save(\"artifacts/labels.npy\", labels)\n",
        "    with open(\"artifacts/zero_shot_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(\"Zero-shot results:\", results)\n",
        "    return image_feats, labels, results"
      ],
      "metadata": {
        "id": "VIrYHfUPPwWy"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluated_prompts = evaluate_prompts(model, test_loader, test, text_prototypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELLM_IdWQN79",
        "outputId": "881c486e-f6ce-42ec-e419-00e9d276a12a"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding images: 100%|| 250/250 [00:27<00:00,  9.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16 torch.float16\n",
            "torch.float16 torch.float16\n",
            "torch.float16 torch.float16\n",
            "Zero-shot results: {'plain': {'acc': 95.375}, 'short': {'acc': 96.72500000000001}, 'variants': {'acc': 96.6125}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def paired_embeddings_for_subset(image_feats_all: torch.Tensor,\n",
        "                                 labels: np.ndarray,\n",
        "                                 text_protos_for_analysis: torch.Tensor,\n",
        "                                 n_samples: int = 100,\n",
        "                                 seed: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idxs = rng.choice(len(labels), size=min(n_samples, len(labels)), replace=False)\n",
        "    X = image_feats_all[idxs]                         # [k,D]\n",
        "    Y = text_protos_for_analysis[labels[idxs].astype(int)]        # ground-truth text proto for each sample\n",
        "    return idxs, X.cpu().numpy(), Y.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "ESRXNozjQN3M"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paired_subset_embeddings=paired_embeddings_for_subset(encoded_images, test.labels, text_prototypes['variants'], n_samples=100, seed=0)"
      ],
      "metadata": {
        "id": "a_lavaKkQN0f"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idxs, X, Y = paired_subset_embeddings\n",
        "print(f\"Size of indices: {idxs.size}\")\n",
        "print(f\"Shape of image features subset: {X.shape}\")\n",
        "print(f\"Shape of text prototypes subset: {Y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMOYjcQrQNvG",
        "outputId": "49b0db4a-d64e-446b-b413-8f7a88d4af80"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of indices: 100\n",
            "Shape of image features subset: (100, 512)\n",
            "Shape of text prototypes subset: (100, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D projections (t-SNE / UMAP)\n",
        "def project_2d(X: np.ndarray, Y: np.ndarray, method=\"tsne\", seed=0):\n",
        "    Z = np.concatenate([X, Y], axis=0)\n",
        "    if method == \"umap\" and HAVE_UMAP:\n",
        "        reducer = umap.UMAP(random_state=seed, n_neighbors=15, min_dist=0.1)\n",
        "        Z2 = reducer.fit_transform(Z)\n",
        "    else:\n",
        "        Z2 = TSNE(n_components=2, random_state=seed, init=\"pca\", perplexity=min(30, max(5, len(Z)//10))).fit_transform(Z)\n",
        "    X2 = Z2[:len(X)]\n",
        "    Y2 = Z2[len(X):]\n",
        "    return X2, Y2"
      ],
      "metadata": {
        "id": "sdQFMhqVQNrq"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_2d_tsne=project_2d(X, Y, method=\"tsne\", seed=0)\n",
        "project_2d_umap=project_2d(X, Y, method=\"umap\", seed=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dG_UzUJQNoA",
        "outputId": "abd95647-6859-4e8a-c687-628fd7b2b9f9"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of image features subset: {project_2d_tsne[0].shape}\")\n",
        "print(f\"Shape of text prototypes subset: {project_2d_tsne[1].shape}\")\n",
        "print(f\"Shape of image features subset: {project_2d_umap[0].shape}\")\n",
        "print(f\"Shape of text prototypes subset: {project_2d_umap[1].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMWJ0x6VT9Ip",
        "outputId": "7cadd9e3-5f57-4887-9a00-03081e2b99d2"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of image features subset: (100, 2)\n",
            "Shape of text prototypes subset: (100, 2)\n",
            "Shape of image features subset: (100, 2)\n",
            "Shape of text prototypes subset: (100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_modalities(X2, Y2, title, fname):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(X2[:,0], X2[:,1], s=16, alpha=0.7, label=\"image feats\")\n",
        "    plt.scatter(Y2[:,0], Y2[:,1], s=16, alpha=0.7, marker=\"x\", label=\"text feats\")\n",
        "    plt.legend(); plt.title(title); plt.tight_layout()\n",
        "    plt.savefig(fname, dpi=150); plt.close()"
      ],
      "metadata": {
        "id": "ru1z38O6UENl"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_modalities_tsne = plot_modalities(project_2d_tsne[0], project_2d_tsne[1], \"Pre-alignment (t-SNE)\", \"artifacts/modality_gap_pre_tsne.png\")\n",
        "plot_modalities_umap = plot_modalities(project_2d_tsne[0], project_2d_tsne[1], \"Pre-alignment (UMAP)\", \"artifacts/modality_gap_pre_umap.png\")"
      ],
      "metadata": {
        "id": "Ha4xuQPmUFls"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Procrustes alignment (orthogonal)\n",
        "def orthogonal_align(X: np.ndarray, Y: np.ndarray, l2norm_before=True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    Xc = X.copy(); Yc = Y.copy()\n",
        "    if l2norm_before:\n",
        "        Xc = Xc / (np.linalg.norm(Xc, axis=1, keepdims=True) + 1e-9)\n",
        "        Yc = Yc / (np.linalg.norm(Yc, axis=1, keepdims=True) + 1e-9)\n",
        "    R, _ = orthogonal_procrustes(Xc, Yc)  # finds R minimizing ||X R - Y||_F  (R is orthogonal)\n",
        "    X_aligned = X @ R\n",
        "    return X_aligned, R"
      ],
      "metadata": {
        "id": "vRzHIvEEUFi_"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_aligned, R = orthogonal_align(X, Y, l2norm_before=True)"
      ],
      "metadata": {
        "id": "31xvSL03UFgO"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of image features subset: {X_aligned.shape}\")\n",
        "print(f\"Shape of text prototypes subset: {R.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbw2x1eKUFdu",
        "outputId": "23dadf2f-56e2-4b72-f134-e3caf833d2ea"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of image features subset: (100, 512)\n",
            "Shape of text prototypes subset: (512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def apply_rotation_to_all(image_feats_all: torch.Tensor, R: np.ndarray) -> torch.Tensor:\n",
        "    A = torch.from_numpy(R).to(image_feats_all.device).float()  # [D,D]\n",
        "    return (image_feats_all @ A.T).contiguous()"
      ],
      "metadata": {
        "id": "mpaohFXZU3Ry"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotated_images = apply_rotation_to_all(encoded_images, R)"
      ],
      "metadata": {
        "id": "J7huEqZJU4nu"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of image features subset: {rotated_images.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rYGvRA7U9Ny",
        "outputId": "0627c458-258c-456d-b4db-ba41d9fed359"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of image features subset: torch.Size([8000, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# (1) Build prompt sets & text prototypes\n",
        "prompt_sets = prompt_sets()\n",
        "text_protos = build_text_prototypes(model, prompt_sets)  # dict name -> [C,D]\n",
        "# Add also a reference text proto bank well use for pairing/visualization (take the strongest = variants)\n",
        "text_proto_for_analysis = text_protos[\"variants\"]  # [C,D]\n",
        "\n",
        "image_feats_all, labels, results0 = evaluate_prompts(model, test_loader, test, text_protos)\n",
        "\n",
        "idxs, X_im, Y_txt = paired_embeddings_for_subset(encoded_images, labels, text_proto_for_analysis, n_samples=100, seed=0)\n",
        "for meth in [\"tsne\", \"umap\"]:\n",
        "  X2, Y2 = project_2d(X_im, Y_txt, method=meth, seed=0)\n",
        "  plot_modalities(X2, Y2, f\"Pre-alignment ({meth.upper()})\", f\"artifacts/modality_gap_pre_{meth}.png\")\n",
        "X_aligned_norm, R_norm = orthogonal_align(X_im, Y_txt, l2norm_before=True)\n",
        "X2a, Y2a = project_2d(X_aligned_norm, Y_txt, method=\"tsne\", seed=0)\n",
        "plot_modalities(X2a, Y2a, \"Post-alignment (t-SNE, L2 before Procrustes)\", \"artifacts/modality_gap_post_tsne_norm.png\")\n",
        "\n",
        "X_aligned_raw, R_raw = orthogonal_align(X_im, Y_txt, l2norm_before=False)\n",
        "X2b, Y2b = project_2d(X_aligned_raw, Y_txt, method=\"tsne\", seed=0)\n",
        "plot_modalities(X2b, Y2b, \"Post-alignment (t-SNE, NO L2 before Procrustes)\", \"artifacts/modality_gap_post_tsne_noL2.png\")\n",
        "\n",
        "# (3f) Recompute zero-shot accuracy with aligned image features (use same text prototypes)\n",
        "# We apply R learned on subset to ALL image features, then evaluate\n",
        "img_feats_rot_norm = apply_rotation_to_all(image_feats_all, R_norm)\n",
        "img_feats_rot_raw  = apply_rotation_to_all(image_feats_all, R_raw)\n",
        "\n",
        "aligned_results = {}\n",
        "for name, protos in text_protos.items():\n",
        "    acc_norm, _ = zero_shot_eval(img_feats_rot_norm, protos.to(img_feats_rot_norm.device), labels)\n",
        "    acc_raw,  _  = zero_shot_eval(img_feats_rot_raw,  protos.to(img_feats_rot_raw.device),  labels)\n",
        "    aligned_results[name] = {\"acc_procrustes_L2\": acc_norm, \"acc_procrustes_noL2\": acc_raw}\n",
        "\n",
        "with open(\"artifacts/zero_shot_aligned_results.json\", \"w\") as f:\n",
        "    json.dump(aligned_results, f, indent=2)\n",
        "\n",
        "print(\"\\n=== Baseline zero-shot (%) ===\")\n",
        "for k,v in results0.items(): print(f\"{k:10s}: {v['acc']:.2f}\")\n",
        "\n",
        "print(\"\\n=== After Procrustes (train on subset, apply to all) ===\")\n",
        "for k,v in aligned_results.items():\n",
        "    print(f\"{k:10s}: L2 {v['acc_procrustes_L2']:.2f} | noL2 {v['acc_procrustes_noL2']:.2f}\")\n",
        "\n",
        "# Save report\n",
        "report = {\n",
        "    \"baseline\": results0,\n",
        "    \"aligned\": aligned_results,\n",
        "    \"notes\": [\n",
        "        \"CLIP features are L2-normalized; cosine similarity is dot product.\",\n",
        "        \"Prompt ensembling (variants) typically > short > plain.\",\n",
        "        \"Procrustes is orthogonal (rotation/reflection); preserves norms and pairwise distances.\",\n",
        "        \"L2-normalizing features before Procrustes often yields better alignment (consistent scale).\"\n",
        "    ]\n",
        "}\n",
        "with open(\"artifacts/report_summary.json\", \"w\") as f:\n",
        "    json.dump(report, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqNA2Vk8YxQT",
        "outputId": "9837bd82-4163-4334-f85c-056d28b36d27"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding images: 100%|| 250/250 [00:26<00:00,  9.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16 torch.float16\n",
            "torch.float16 torch.float16\n",
            "torch.float16 torch.float16\n",
            "Zero-shot results: {'plain': {'acc': 95.375}, 'short': {'acc': 96.72500000000001}, 'variants': {'acc': 96.6125}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16 torch.float16\n",
            "torch.float16 torch.float16\n",
            "torch.float16 torch.float16\n",
            "torch.float16 torch.float16\n",
            "torch.float16 torch.float16\n",
            "torch.float16 torch.float16\n",
            "\n",
            "=== Baseline zero-shot (%) ===\n",
            "plain     : 95.38\n",
            "short     : 96.73\n",
            "variants  : 96.61\n",
            "\n",
            "=== After Procrustes (train on subset, apply to all) ===\n",
            "plain     : L2 59.70 | noL2 64.45\n",
            "short     : L2 80.99 | noL2 68.19\n",
            "variants  : L2 80.44 | noL2 70.89\n"
          ]
        }
      ]
    }
  ]
}